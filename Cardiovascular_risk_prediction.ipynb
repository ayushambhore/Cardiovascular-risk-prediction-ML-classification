{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW",
        "O_i_v8NEhb9l",
        "HhfV-JJviCcP",
        "Y3lxredqlCYt",
        "3RnN4peoiCZX",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "u3PMJOP6ngxN",
        "dauF4eBmngu3",
        "bKJF3rekwFvQ",
        "MSa1f5Uengrz",
        "GF8Ens_Soomf",
        "0wOQAZs5pc--",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "KSlN3yHqYklG",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "EM7whBJCYoAo",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "bamQiAODYuh1",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "OH-pJp9IphqM",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "PIIx-8_IphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "BZR9WyysphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "U2RJ9gkRphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "x-EpHcCOp1ci",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "Ag9LCva-p1cl",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "g-ATYxFrGrvw",
        "Yfr_Vlr8HBkt",
        "8yEUt7NnHlrM",
        "tEA2Xm5dHt1r",
        "I79__PHVH19G",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "4_0_7-oCpUZd",
        "hwyV_J3ipUZe",
        "3yB-zSqbpUZe",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "bn_IUdTipZyH",
        "49K5P_iCpZyH",
        "Nff-vKELpZyI",
        "kLW572S8pZyI",
        "dWbDXHzopZyI",
        "yLjJCtPM0KBk",
        "xiyOF9F70UgQ",
        "7wuGOrhz0itI",
        "id1riN9m0vUs",
        "578E2V7j08f6",
        "89xtkJwZ18nB",
        "67NQN5KX2AMe",
        "Iwf50b-R2tYG",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "-oLEiFgy-5Pf",
        "C74aWNz2AliB",
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "TNVZ9zx19K6k",
        "nqoHp30x9hH9",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "BhH2vgX9EjGr",
        "qjKvONjwE8ra",
        "P1XJ9OREExlT",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "VfCC591jGiD4",
        "OB4l2ZhMeS1U",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayushambhore/Cardiovascular-risk-prediction-ML-classification/blob/main/Cardiovascular_risk_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - Cardiovascular Risk Prediction\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Classification\n",
        "##### **Contribution**    - Individual\n",
        "##### **Name -** Ayush Ambhore"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This project focused on the prediction of Coronary Heart Disease (CHD) risk based on a dataset containing demographic, behavioral, and medical history features of patients. The dataset included information on gender, age, smoking status, blood pressure medication, previous stroke history, hypertension, diabetes prevalence, and various medical measurements.\n",
        "\n",
        "The initial exploratory data analysis revealed interesting insights, such as a slightly higher number of females in the dataset and more non-smokers than smokers. Age was found to be a significant risk factor for CHD, and certain medical measurements like cholesterol levels, blood pressure, BMI, heart rate, and glucose levels showed distinct patterns.\n",
        "\n",
        "To preprocess the data, missing values were handled by dropping rows with missing values, and outliers were treated using the Interquartile Range (IQR) method for continuous variables. Categorical variables were encoded, and feature manipulation and selection were performed to enhance the model's performance.\n",
        "\n",
        "The dataset was then split into training and testing sets, and to handle the class imbalance issue, the Synthetic Minority Over-sampling Technique (SMOTE) was applied.\n",
        "\n",
        "Eight different machine learning models, including Logistic Regression, Decision Tree, Random Forest, K-Nearest Neighbors (KNN), Support Vector Machine (SVM), and XGBoost, were trained and evaluated on the dataset.\n",
        "\n",
        "The evaluation results demonstrated promising performance for all models, with high accuracy, precision, recall, F1 score, and ROC AUC values on both training and test datasets.\n",
        "\n",
        "The Logistic Regression model displayed stable performance with no signs of overfitting, making it a reliable choice for the task.\n",
        "\n",
        "The Decision Tree model exhibited perfect accuracy on the training dataset but slightly worse performance on the test dataset, indicating some overfitting.\n",
        "\n",
        "The Random Forest model demonstrated robust performance on both datasets but also showed signs of overfitting.\n",
        "\n",
        "The KNN model achieved perfect accuracy on the training dataset, but its performance on the test dataset was still promising, making it a viable option.\n",
        "\n",
        "The SVM model demonstrated stable and consistent performance on both datasets, showcasing good generalization capabilities.\n",
        "\n",
        "The XGBoost model emerged as the best-performing model with high accuracy, precision, recall, F1 score, and ROC AUC values on both datasets. It exhibited strong generalization capabilities and minimal overfitting, making it the most reliable and efficient model for this classification task.\n",
        "\n",
        "In summary, this project successfully developed and evaluated machine learning models to predict the risk of Coronary Heart Disease. The XGBoost model proved to be the most robust and reliable model, offering accurate and precise predictions. The insights gained from this project could be valuable for medical practitioners and policymakers to identify individuals at higher risk of CHD and implement preventive measures accordingly. However, further research and analysis with a larger dataset could enhance the model's performance and provide more comprehensive insights into CHD risk factors.\n"
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/ayushambhore/Cardiovascular-risk-prediction-ML-classification"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The goal of this project is to develop a machine learning model that can accurately predict the 10-year risk of Coronary Heart Disease (CHD) based on various demographic, behavioral, and medical history features of patients. The dataset contains information on gender, age, smoking status, blood pressure medication, previous stroke history, hypertension, diabetes prevalence, and medical measurements such as cholesterol levels, blood pressure, BMI, heart rate, and glucose levels."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from numpy import math\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Importing libraries for modelling and evaluation\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from collections import Counter\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, roc_auc_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "import xgboost as xgb\n",
        "\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "url = 'https://raw.githubusercontent.com/ayushambhore/Cardiovascular-risk-prediction-ML-classification/main/data_cardiovascular_risk.csv'\n",
        "df = pd.read_csv(url)"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "df.head(10)"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "print(f' Row count = {df.shape[0]}\\n Column count = {df.shape[1]}')"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "len(df[df.duplicated()])"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values\n",
        "print(df.isnull().sum())"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.isnull().sum().sum())"
      ],
      "metadata": {
        "id": "8zJuBwtJ8zZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "sns.heatmap(df.isnull(), cbar=False);"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This data refers to the information collected about the functioning and health of the heart. The dataset have 3390 rows and 17 columns. The data have no duplicate rows and 510 missing values."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "df.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "df.describe(include='all')"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Demographic:\n",
        "\n",
        "* Gender: Indicating the biological sex of the patient (categorical: \"Male\" or \"Female\").\n",
        "* Age: The patient's age, measured in years (continuous variable).\n",
        "Behavioral:\n",
        "\n",
        "* Smoking Status: Whether or not the patient is a current smoker (categorical: \"Yes\" or \"No\").\n",
        "* Average Cigarettes Per Day: The average number of cigarettes smoked per day by the patient (continuous variable).\n",
        "\n",
        "Medical History:\n",
        "\n",
        "* Blood Pressure Medication: Whether or not the patient is currently taking blood pressure medication (categorical: \"Yes\" or \"No\").\n",
        "* Previous Stroke: Whether or not the patient has previously experienced a stroke (categorical: \"Yes\" or \"No\").\n",
        "* Hypertension Prevalence: Whether or not the patient has been diagnosed with hypertension (categorical: \"Yes\" or \"No\").\n",
        "* Diabetes Prevalence: Whether or not the patient has been diagnosed with diabetes (categorical: \"Yes\" or \"No\").\n",
        "\n",
        "Current Medical Measurements:\n",
        "\n",
        "* Total Cholesterol Level: The patient's total cholesterol level (continuous variable).\n",
        "* Systolic Blood Pressure: The patient's systolic blood pressure reading (continuous variable).\n",
        "* Diastolic Blood Pressure: The patient's diastolic blood pressure reading (continuous variable).\n",
        "* BMI (Body Mass Index): The patient's calculated body mass index (continuous variable).\n",
        "* Heart Rate: The patient's heart rate (continuous variable).\n",
        "* Glucose Level: The patient's glucose level (continuous variable).\n",
        "\n",
        "Predicted Variable (Target):\n",
        "\n",
        "* 10-year Risk of Coronary Heart Disease (CHD): Indicating the likelihood of the patient developing coronary heart disease within the next 10 years (binary: \"1\" for \"Yes\" and \"0\" for \"No\").\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "for i in df.columns.tolist():\n",
        "  print(\"No. of unique values in \",i,\"is\",df[i].nunique(),\".\")"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***3. EDA***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Univariate analysis of Categorical Variables"
      ],
      "metadata": {
        "id": "45Pd140U1OA3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_var = ['sex','is_smoking','education','BPMeds','prevalentStroke', 'prevalentHyp', 'diabetes']\n",
        "\n",
        "# Create a figure to hold the subplots\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "# Set the title for the entire figure\n",
        "plt.suptitle('Exploring Categorical Features', fontsize=20, fontweight='bold', y=1.02)\n",
        "\n",
        "# Iterate through the categorical features and create individual subplots\n",
        "for i, col in enumerate(categorical_var):\n",
        "    plt.subplot(3, 3, i+1)  # Create a subplot with 3 rows and 3 columns\n",
        "\n",
        "    # Use seaborn's countplot to visualize the distribution of the current categorical feature\n",
        "    sns.countplot(data=df, x=col, palette=\"tab10\")\n",
        "\n",
        "# Display the plots\n",
        "plt.tight_layout()  # Adjust the layout to prevent overlapping of subplots\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4uFviXd53h5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations**\n",
        "\n",
        "* There are slightly more females than males in the dataset.\n",
        "\n",
        "* The number of non-smokers is slightly higher than the number of smokers, with each group having around 1600 individuals.\n",
        "\n",
        "* Approximately 1500 individuals have an education level of 1, while nearly 400 individuals have an education level of 4. However, the specific definitions of these education levels are not provided.\n",
        "\n",
        "* Over 3000 individuals are not taking medication for blood pressure.\n",
        "\n",
        "* Only a small number of people in the dataset have a history of stroke.\n",
        "\n",
        "* Around 1000 individuals in the dataset have been identified as hypertensive.\n",
        "\n",
        "* A large number, more than 3000 people, do not have diabetes."
      ],
      "metadata": {
        "id": "Y-9Lhn1TTQMv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Univariate analysis of Numerical Variables"
      ],
      "metadata": {
        "id": "IGOkqSnGUBu_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "continuous_var = ['age','cigsPerDay','totChol','sysBP','diaBP','BMI','heartRate','glucose']\n",
        "\n",
        "# Create a figure to hold the subplots\n",
        "plt.figure(figsize=(15, 15))\n",
        "\n",
        "# Iterate through the continuous variables and create individual subplots\n",
        "for i, col in enumerate(continuous_var):\n",
        "    plt.subplot(4, 2, i+1)  # Create a subplot with 3 rows and 3 columns\n",
        "\n",
        "    # Use seaborn's distplot to visualize the distribution of the current continuous variable\n",
        "    sns.distplot(df[col], kde=False, color='skyblue', label='Distribution')\n",
        "\n",
        "    # Calculate mean and median for the current continuous variable\n",
        "    mean_val = df[col].mean()\n",
        "    median_val = df[col].median()\n",
        "\n",
        "    # Add vertical lines to mark mean and median\n",
        "    plt.axvline(mean_val, color='magenta', linestyle='dashed', linewidth=2, label='Mean')\n",
        "    plt.axvline(median_val, color='cyan', linestyle='dashed', linewidth=2, label='Median')\n",
        "\n",
        "    plt.title(col + ' Distribution')\n",
        "    plt.legend()\n",
        "\n",
        "# Display the plots\n",
        "plt.tight_layout()  # Adjust the layout to prevent overlapping of subplots\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "NSz9mTgdVQPs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations**\n",
        "\n",
        "* Age: The age ranges from 35 to 70 years and follows an almost normal distribution. Most individuals belong to the age group around 40 years.\n",
        "\n",
        "* Cigarettes Smoked per Day: On average, most people do not smoke (0 cigarettes per day), but a significant number of individuals smoke 20 cigarettes per day.\n",
        "\n",
        "* Cholesterol: Cholesterol levels range from 100 to 700. However, the majority of individuals have cholesterol levels between 150 and 350.\n",
        "\n",
        "* Systolic Blood Pressure (BP): Systolic BP ranges mainly from 100 to 200.\n",
        "\n",
        "* Diastolic Blood Pressure (BP): Diastolic BP ranges mainly from 60 to 120.\n",
        "\n",
        "* BMI (Body Mass Index): BMI varies from 16 to 40, indicating a range of body weights.\n",
        "\n",
        "* Heart Rate: Heart rate values are observed between 40 to 110 beats per minute. The most common occurrence is around 75 beats per minute.\n",
        "\n",
        "* Glucose: Glucose levels vary from 50 to 125. However, there are some extreme values that cannot be ignored, as they may pose a risk of heart disease."
      ],
      "metadata": {
        "id": "hBFk-3FvYMMx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Bivariate analysis between the dependent variable and continuous independent variables"
      ],
      "metadata": {
        "id": "Fq6PdGiHdD4L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dependent_var = ['TenYearCHD']\n",
        "\n",
        "# Create subplots with a 3x3 grid\n",
        "plt.figure(figsize=(15, 15))\n",
        "plt.subplots_adjust(hspace=0.5)  # Adjust the vertical spacing between subplots\n",
        "\n",
        "# Loop through each continuous variable and create a violin plot\n",
        "for idx, i in enumerate(continuous_var):\n",
        "    plt.subplot(3, 3, idx + 1)  # Create a subplot with 3 rows and 3 columns\n",
        "\n",
        "    # Use seaborn's violinplot to visualize the relationship between dependent_var and the current continuous variable\n",
        "    sns.violinplot(x=dependent_var[0], y=i, data=df, palette={0: \"blue\", 1: \"magenta\"})\n",
        "\n",
        "    # Set labels and title\n",
        "    plt.ylabel(i)\n",
        "    plt.xlabel(dependent_var[0])\n",
        "    plt.title(f\"{dependent_var[0]} vs {i}\")\n",
        "\n",
        "# Display the plots\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "lLFPB3xxcann"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations**\n",
        "\n",
        "* The data analysis indicates a higher risk of Coronary Heart Disease (CHD) among older patients compared to younger ones. However, concerning other continuous variables, there is no definitive evidence supporting their association with CHD risk.\n",
        "\n",
        "* The observations highlight the significance of age as a risk factor for CHD, which aligns with existing medical knowledge. Nonetheless, it is crucial to conduct further research and analysis to explore potential relationships between CHD risk and other continuous variables.\n",
        "\n",
        "\n",
        "Understanding the factors contributing to CHD risk is essential for public health initiatives and individual patient care. Additional investigations can provide valuable insights, leading to better preventive measures and tailored interventions for various risk groups. As the medical landscape evolves, comprehensive data analysis continues to play a vital role in advancing our knowledge of cardiovascular diseases."
      ],
      "metadata": {
        "id": "3ny3t0QljXhC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Bivariate analysis between the dependent variable and categorical independent variables"
      ],
      "metadata": {
        "id": "ZeuC-n05kbfW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the percentage distribution of the dependent variable\n",
        "percent_distribution = df[dependent_var].value_counts(normalize=True) * 100\n",
        "\n",
        "# Create subplots with a layout of 4 rows and 2 columns\n",
        "plt.figure(figsize=(15, 20))\n",
        "plt.subplots_adjust(hspace=0.5, wspace=0.3)  # Adjust the vertical and horizontal spacing between subplots\n",
        "\n",
        "# Loop through each categorical variable and create a 100% stacked bar chart\n",
        "for idx, cat_var in enumerate(categorical_var):\n",
        "    plt.subplot(4, 2, idx + 1)  # Create a subplot with 4 rows and 2 columns\n",
        "\n",
        "    # Calculate the percentage distribution of the dependent variable for the current categorical variable\n",
        "    percent_distribution = df.groupby(cat_var)[dependent_var].value_counts(normalize=True).unstack() * 100\n",
        "\n",
        "    # Create a stacked bar chart for the dependent variable\n",
        "    percent_distribution.plot(kind='bar', stacked=True, ax=plt.gca(), width=0.8)\n",
        "\n",
        "    # Set labels and title\n",
        "    plt.xlabel(cat_var)\n",
        "    plt.ylabel('Percentage')\n",
        "    plt.title(f'Distribution of {dependent_var} by {cat_var}')\n",
        "    plt.xticks(rotation=0)\n",
        "\n",
        "    # Annotate the bars with percentages\n",
        "    for p in plt.gca().patches:\n",
        "        width, height = p.get_width(), p.get_height()\n",
        "        x, y = p.get_xy()\n",
        "        plt.gca().annotate(f'{height:.1f}%', (x + width / 2, y + height / 2), ha='center', va='center')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "76DGf_nnkZpo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations**\n",
        "\n",
        "* Male patients face a significantly higher CHD risk (18%) compared to female patients (12%).\n",
        "\n",
        "* Smoking is associated with a significantly higher CHD risk (16%) compared to non-smoking patients (13%).\n",
        "\n",
        "* Patients with education level 1, 2, 3, and 4 had CHD percentages of 18%, 11%, 12%, and 14%, respectively.\n",
        "\n",
        "* Patients taking BP medicines have a significantly higher CHD risk (33%) than those not on medication (14%).\n",
        "\n",
        "* Patients who experienced a stroke in their life have a substantially higher CHD risk (45%) than other patients (14%).\n",
        "\n",
        "* Hypertensive patients face a significantly higher CHD risk (23%) than non-hypertensive patients (11%).\n",
        "\n",
        "* Diabetic patients have a considerably higher risk of CHD (37%) than other patients (14%)."
      ],
      "metadata": {
        "id": "3MmGbmlxlk3S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### EDA summary\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2ipcHFHizuQI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset exhibits a balanced gender representation, with slightly more females than males. Smoking habits are evenly distributed between smokers and non-smokers. Education levels vary, with a majority at level 1 and fewer at level 4. Many individuals are not taking BP medicines, and stroke history is relatively low. Hypertensive patients and non-diabetics form significant groups. Age follows a normal distribution, with the most common age group around 40 years. Smokers tend to smoke around 20 cigarettes daily. Cholesterol, systolic/diastolic blood pressure, BMI, heart rate, and glucose levels show diverse ranges. Age emerges as a crucial CHD risk factor, while gender, smoking, education levels, BP medicines, stroke history, hypertension, and diabetes are significantly associated with CHD risk.\n"
      ],
      "metadata": {
        "id": "IWVx_E-5z_Ir"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling the NULL values"
      ],
      "metadata": {
        "id": "mW17i006qh5Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the medical dataset we are working with, the values are person-specific and essential for accurate predictions. Therefore, imputing null values using advanced methods could introduce inaccuracies. To ensure data integrity and minimize risks, we have decided on a two-step approach.\n",
        "\n",
        "Firstly, we will identify features with less than 5% null values and drop the corresponding rows. This step ensures that we retain most of the data while removing only a small portion with missing values."
      ],
      "metadata": {
        "id": "dQhdKtZ9sv7t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the percentage of missing values in each column of the DataFrame 'df'\n",
        "missing_percentages = df.isna().sum() / len(df) * 100\n",
        "\n",
        "# Round the missing_percentages to 2 decimal places\n",
        "rounded_missing_percentages = missing_percentages.round(2)\n",
        "\n",
        "# Print the rounded missing percentages for each column\n",
        "print(\"Missing value percentages:\")\n",
        "print(rounded_missing_percentages)\n"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.dropna(subset=['education', 'cigsPerDay', 'BPMeds', 'totChol', 'BMI', 'heartRate'], inplace=True)"
      ],
      "metadata": {
        "id": "_n2pAp2-sVLu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Secondly, for the remaining rows, we will perform imputation to fill in the null values. Although this step may introduce some level of uncertainty in the predictions, we believe it will not significantly affect the overall outcome."
      ],
      "metadata": {
        "id": "AqtyZP95tXKT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Impute missing values in 'glucose' column with the median value from dataset\n",
        "median_glucose = df['glucose'].median()\n",
        "df['glucose'] = df['glucose'].fillna(median_glucose)"
      ],
      "metadata": {
        "id": "VV2JT6_wtbCv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "sns.heatmap(df.isnull(), cbar=False);"
      ],
      "metadata": {
        "id": "RTkpPZ6Ftzos"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Q1 = df.quantile(0.25)\n",
        "Q3 = df.quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "outliers_count = ((df < Q1 - 1.5 * IQR) | (df > Q3 + 1.5 * IQR)).sum(axis=0)\n",
        "\n",
        "outliers_count"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that very less collinearity is present so we should treat these ."
      ],
      "metadata": {
        "id": "NDVNZg0kc7zU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for col in continuous_var:\n",
        "    # Step 1: Calculate the lower and upper thresholds for outliers using the Interquartile Range (IQR).\n",
        "    lower_threshold = df[col].quantile(0.25) - 1.5 * IQR[col]\n",
        "    upper_threshold = df[col].quantile(0.75) + 1.5 * IQR[col]\n",
        "\n",
        "    # Step 2: Replace the values below the lower threshold with the lower threshold value itself.\n",
        "    df[col] = df[col].mask(df[col] < lower_threshold, lower_threshold)\n",
        "\n",
        "    # Step 3: Replace the values above the upper threshold with the upper threshold value itself.\n",
        "    df[col] = df[col].mask(df[col] > upper_threshold, upper_threshold)\n"
      ],
      "metadata": {
        "id": "eeJBLsu93Dry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q1 = df.quantile(0.25)\n",
        "Q3 = df.quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "outliers_count = ((df < Q1 - 1.5 * IQR) | (df > Q3 + 1.5 * IQR)).sum(axis=0)\n",
        "\n",
        "outliers_count"
      ],
      "metadata": {
        "id": "e9HKj9Ba3HUY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used the Interquartile Range (IQR) method to handle outliers. This technique is robust, non-parametric, and simple. It calculates the IQR, determines lower and upper thresholds, and replaces potential outliers with the corresponding threshold values using the mask function. By using the IQR method, extreme values are identified and treated, improving data accuracy and analysis reliability."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode 'is_smoking' column with 'YES' as 1 and 'NO' as 0\n",
        "df['is_smoking'] = df['is_smoking'].map({'YES': 1, 'NO': 0})\n",
        "\n",
        "# Encode 'sex' column with 'M' as 1 and 'F' as 0\n",
        "df['sex'] = df['sex'].map({'M': 1, 'F': 0})"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "we will perform one-hot encoding on the 'education' feature in the DataFrame. This process will convert the categorical variable 'education' into binary columns for each unique category, and we will then drop the original 'education' feature to avoid redundancy."
      ],
      "metadata": {
        "id": "4QWc-GnsMMr9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform one-hot encoding for the 'education' feature\n",
        "education_onehot = pd.get_dummies(df['education'], prefix='edu')\n",
        "\n",
        "# Drop the original 'education' feature from the DataFrame\n",
        "df = df.drop('education', axis=1)\n",
        "\n",
        "# Concatenate the one-hot encoded 'education' feature with the rest of the data\n",
        "df = pd.concat([df, education_onehot], axis=1)\n",
        "\n",
        "# Display the first three rows of the updated DataFrame\n",
        "df.head(3)\n"
      ],
      "metadata": {
        "id": "GDSE7dCDDD38"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code calculates the 'pulse_pressure' by subtracting 'diaBP' from 'sysBP' and then drops the original 'sysBP' and 'diaBP' columns from the DataFrame."
      ],
      "metadata": {
        "id": "PwEBUKOrOvJO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new column 'pulse_pressure' by calculating the pulse pressure as the difference between 'sysBP' and 'diaBP'\n",
        "df['pulse_pressure'] = df['sysBP'] - df['diaBP']\n",
        "\n",
        "# Drop the 'sysBP' and 'diaBP' columns from the DataFrame\n",
        "df = df.drop(['sysBP', 'diaBP'], axis=1)"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(20,15))\n",
        "heatmap = sns.heatmap(df.corr(), cmap=\"YlGnBu\", annot=True)\n",
        "\n",
        "heatmap.set_title('Correlation Heatmap');"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# droping is_smoking column due to multi-collinearity\n",
        "df.drop('is_smoking', axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "RkRG6Cf_vqvE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "continuous_var = ['age','cigsPerDay','totChol','BMI','heartRate','glucose']\n",
        "# skewness along the index axis\n",
        "(df[continuous_var]).skew(axis = 0)"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Skew for log10 transformation\n",
        "np.log10(df[continuous_var]+1).skew(axis = 0)"
      ],
      "metadata": {
        "id": "pmN2kRAIwi4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The log transformation of the continuous variables has noticeably reduced their skewness."
      ],
      "metadata": {
        "id": "Vgbqh3E4w2mv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for col in continuous_var:\n",
        "  # Applying log transformation to the column values after adding 1 to avoid negative values (since log(0) is undefined).\n",
        "    df[col] = np.log10(df[col] + 1)"
      ],
      "metadata": {
        "id": "TmSDyY9Ww3cR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.log10(df[continuous_var]+1).skew(axis = 0)"
      ],
      "metadata": {
        "id": "jzk8Uy4pxCpT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Splitting"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y = df['TenYearCHD'] #dependent variable"
      ],
      "metadata": {
        "id": "kpj7kl1CzT2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.drop(columns='TenYearCHD') #independent variables"
      ],
      "metadata": {
        "id": "CdIIc3IyzZz6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dependant Column Value Counts\n",
        "print(df['TenYearCHD'].value_counts())\n",
        "print(\" \")\n",
        "\n",
        "# Dependant Variable Column Visualization - Pie Chart\n",
        "plt.figure(figsize=(6, 6))\n",
        "df['TenYearCHD'].value_counts().plot(kind='pie', autopct=\"%1.1f%%\", startangle=90)\n",
        "plt.ylabel('')\n",
        "plt.title('TenYearCHD Distribution')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "gYeYUUgY0R-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes"
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display class distribution before handling imbalance\n",
        "class_distribution_before = Counter(y)\n",
        "print(f'Before Handling Imbalanced class: {class_distribution_before}')\n",
        "\n",
        "# Resample the minority class using SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "X, y = smote.fit_resample(X, y)\n",
        "\n",
        "# Display class distribution after handling imbalance\n",
        "class_distribution_after = Counter(y)\n",
        "print(f'After Handling Imbalanced class: {class_distribution_after}')\n"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2, random_state = 0)"
      ],
      "metadata": {
        "id": "2H5KADrr2HY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape)\n",
        "print(X_test.shape)"
      ],
      "metadata": {
        "id": "9PerfuDu2hc5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data scaling"
      ],
      "metadata": {
        "id": "1unLyabg3UkG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling Data\n",
        "scaler = StandardScaler()\n",
        "\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "zhQ9JM053aQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation metrics"
      ],
      "metadata": {
        "id": "aqDIQMkJ3PZr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the columns for the evaluation_results DataFrame\n",
        "columns = ['Model', 'Train Accuracy', 'Test Accuracy', 'Train Precision', 'Test Precision',\n",
        "           'Train Recall', 'Test Recall', 'Train F1 Score', 'Test F1 Score',\n",
        "           'Train ROC AUC', 'Test ROC AUC']\n",
        "\n",
        "# Create an empty DataFrame to store the evaluation results of different models\n",
        "evaluation_results = pd.DataFrame(columns=columns)\n",
        "\n",
        "# Define a function to evaluate a model and store its performance metrics in the evaluation_results DataFrame\n",
        "def evaluate_model(y_train, y_train_pred, y_test, y_test_pred, model_name):\n",
        "    global evaluation_results  # Use the global evaluation_results DataFrame\n",
        "\n",
        "    # Calculate various evaluation metrics for the training and test sets\n",
        "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
        "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "    train_precision = precision_score(y_train, y_train_pred)\n",
        "    test_precision = precision_score(y_test, y_test_pred)\n",
        "    train_recall = recall_score(y_train, y_train_pred)\n",
        "    test_recall = recall_score(y_test, y_test_pred)\n",
        "    train_f1 = f1_score(y_train, y_train_pred)\n",
        "    test_f1 = f1_score(y_test, y_test_pred)\n",
        "    train_roc_auc = roc_auc_score(y_train, y_train_pred)\n",
        "    test_roc_auc = roc_auc_score(y_test, y_test_pred)\n",
        "\n",
        "    # Create a DataFrame to store the evaluation metrics for the current model\n",
        "    model_evaluation = pd.DataFrame({\n",
        "        'Model': [model_name],\n",
        "        'Train Accuracy': [train_accuracy],\n",
        "        'Test Accuracy': [test_accuracy],\n",
        "        'Train Precision': [train_precision],\n",
        "        'Test Precision': [test_precision],\n",
        "        'Train Recall': [train_recall],\n",
        "        'Test Recall': [test_recall],\n",
        "        'Train F1 Score': [train_f1],\n",
        "        'Test F1 Score': [test_f1],\n",
        "        'Train ROC AUC': [train_roc_auc],\n",
        "        'Test ROC AUC': [test_roc_auc]\n",
        "    })\n",
        "\n",
        "    # Concatenate the model_evaluation DataFrame with the global evaluation_results DataFrame\n",
        "    # This step appends the results of the current model to the overall evaluation_results DataFrame\n",
        "    evaluation_results = pd.concat([evaluation_results, model_evaluation], ignore_index=True)\n",
        "\n",
        "    # Print the evaluation metrics for the current model\n",
        "    print(f\"Evaluation Metrics for {model_name}:\")\n",
        "    print(f\"Train Accuracy: {train_accuracy:.4f}\")\n",
        "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "    print(f\"Train Precision: {train_precision:.4f}\")\n",
        "    print(f\"Test Precision: {test_precision:.4f}\")\n",
        "    print(f\"Train Recall: {train_recall:.4f}\")\n",
        "    print(f\"Test Recall: {test_recall:.4f}\")\n",
        "    print(f\"Train F1 Score: {train_f1:.4f}\")\n",
        "    print(f\"Test F1 Score: {test_f1:.4f}\")\n",
        "    print(f\"Train ROC AUC: {train_roc_auc:.4f}\")\n",
        "    print(f\"Test ROC AUC: {test_roc_auc:.4f}\")"
      ],
      "metadata": {
        "id": "DV2bC1Kpxu3V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to plot the confusion matrix\n",
        "def plot_confusion_matrix(y_true, y_pred, labels):\n",
        "    # Calculate the confusion matrix using sklearn's confusion_matrix function\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
        "\n",
        "    # Create a DataFrame to visualize the confusion matrix with row and column labels\n",
        "    cm_df = pd.DataFrame(cm, index=labels, columns=labels)\n",
        "\n",
        "    # Set the size of the plot\n",
        "    plt.figure(figsize=(4, 3))  # You can adjust the figsize to make the plot smaller or larger\n",
        "\n",
        "    # Create a heatmap using seaborn to visualize the confusion matrix\n",
        "    sns.heatmap(cm_df, annot=True, fmt='d', cmap='Blues', annot_kws={\"size\": 8})\n",
        "    # The 'annot=True' parameter adds the numerical values to the heatmap.\n",
        "    # The 'fmt='d'' parameter formats the values as integers in the heatmap.\n",
        "    # The 'cmap='Blues'' parameter sets the color map to shades of blue.\n",
        "    # The 'annot_kws={\"size\": 8}' parameter adjusts the font size for the annotations.\n",
        "\n",
        "    # Set the x-axis label and its font size\n",
        "    plt.xlabel('Predicted Labels', fontsize=10)\n",
        "\n",
        "    # Set the y-axis label and its font size\n",
        "    plt.ylabel('True Labels', fontsize=10)\n",
        "\n",
        "    # Set the title of the plot and its font size\n",
        "    plt.title('Confusion Matrix', fontsize=12)\n",
        "\n",
        "    # Display the plot\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "cX1hcqMq0K9y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model-1 : **Logistic regression**"
      ],
      "metadata": {
        "id": "QOQDH2Shq9pw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fitting model\n",
        "logr_model = LogisticRegression()\n",
        "# training the model\n",
        "logr_model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "eoPXDATFrCEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train predictions\n",
        "logr_train_pred = logr_model.predict(X_train)"
      ],
      "metadata": {
        "id": "xMeIC8mUrWCA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training set recall\n",
        "logr_train_recall = recall_score(y_train,logr_train_pred)\n",
        "logr_train_recall"
      ],
      "metadata": {
        "id": "73EdALQKrgWt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test predictions\n",
        "logr_test_pred = logr_model.predict(X_test)"
      ],
      "metadata": {
        "id": "BJFCYFnOsb58"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test recall\n",
        "logr_test_recall = recall_score(y_test,logr_test_pred)\n",
        "logr_test_recall"
      ],
      "metadata": {
        "id": "LLe9Qgnlsk3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_model(y_train,logr_train_pred,y_test,logr_test_pred,'Logistic regression')"
      ],
      "metadata": {
        "id": "0kPgGHPpwBTX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_confusion_matrix(y_test,logr_test_pred,[True,False])"
      ],
      "metadata": {
        "id": "RJmnkS2q0RjF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The Logistic Regression model achieved an 80.6% training accuracy and 79.9% test accuracy.\n",
        "* It demonstrated good precision with 86.3% for training and 84.8% for testing, indicating its ability to correctly identify positive cases.\n",
        "* The recall values were 72.7% for training and 72.9% for testing, suggesting that it captures a reasonable portion of actual positive cases.\n",
        "* The model's F1 scores were 78.9% for training and 78.4% for testing, which represent a balance between precision and recall.\n",
        "* Its ROC AUC scores were 80.6% for training and 79.9% for testing, highlighting its ability to discriminate between positive and negative classes."
      ],
      "metadata": {
        "id": "E-ZgnXpWZsXl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML model 2: **Decision Tree**"
      ],
      "metadata": {
        "id": "bsUNq-zoe4uW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fitting model\n",
        "DT_model = DecisionTreeClassifier()\n",
        "# training the model\n",
        "DT_model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "_bqWH01SfVu0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train predictions\n",
        "DT_train_pred = DT_model.predict(X_train)"
      ],
      "metadata": {
        "id": "sSC30B_Af6EE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training set recall\n",
        "DT_train_recall = recall_score(y_train,DT_train_pred)\n",
        "DT_train_recall"
      ],
      "metadata": {
        "id": "F1kWMWgxf_60"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test predictions\n",
        "DT_test_pred = DT_model.predict(X_test)"
      ],
      "metadata": {
        "id": "CNoXSpbmhVuY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test recall\n",
        "DT_test_recall = recall_score(y_test,DT_test_pred)\n",
        "DT_test_recall"
      ],
      "metadata": {
        "id": "RDC-e9uehb-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_model(y_train,DT_train_pred,y_test,DT_test_pred,'Decision Tree')"
      ],
      "metadata": {
        "id": "NTfdgHD1hlkN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_confusion_matrix(y_test,DT_test_pred,[True,False])"
      ],
      "metadata": {
        "id": "ku3reog1hzC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The Decision Tree model demonstrated perfect training accuracy of 100% and good testing accuracy of 82.6%.\n",
        "* It achieved perfect precision of 100% on the training set and 81.5% on the testing set, indicating its ability to correctly classify positive instances.\n",
        "* The model showed perfect recall of 100% in training and 84.3% in testing, indicating that it effectively identifies true positive cases.\n",
        "* With a perfect F1 score of 100% in training and 82.9% in testing, it strikes a balance between precision and recall.\n",
        "* The model displayed excellent discriminative power with perfect ROC AUC of 100% in training and 82.6% in testing."
      ],
      "metadata": {
        "id": "ctzfElaKaKmt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model 3: Random Forest"
      ],
      "metadata": {
        "id": "cwV4kFyTituZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a RandomForestClassifier object\n",
        "random_forest = RandomForestClassifier()\n",
        "\n",
        "# Define the grid of hyperparameters to search\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 80,  100],  # Number of trees in the ensemble\n",
        "    'max_features': [\"log2\", \"sqrt\"],  # Maximum number of features considered when splitting a node\n",
        "    'max_depth': [10, 15, 20],  # Maximum number of levels allowed in each tree\n",
        "    'min_samples_split': [2, 5, 10],  # Minimum number of samples necessary in a node to cause node splitting\n",
        "    'min_samples_leaf': [1, 2, 4]  # Minimum number of samples required in a leaf node\n",
        "}\n",
        "\n",
        "# Create a GridSearchCV object\n",
        "random_forest_cv = GridSearchCV(random_forest, param_grid=param_grid, scoring='roc_auc', cv=5)\n",
        "\n",
        "# Fit the GridSearchCV object to the training dataset\n",
        "random_forest_cv.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "vPncVh4vjVms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RF_test_pred = random_forest_cv.predict(X_test)\n",
        "RF_train_pred = random_forest_cv.predict(X_train)"
      ],
      "metadata": {
        "id": "QRh-_5NgkGTk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_model(y_train,RF_train_pred,y_test,RF_test_pred,'Random Forest')"
      ],
      "metadata": {
        "id": "ce2YRYBLj8Er"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_confusion_matrix(y_test,RF_test_pred,[True,False])"
      ],
      "metadata": {
        "id": "EVcjkCqrkR96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The Random Forest model achieved perfect training accuracy of 100% and high testing accuracy of 87.9%, indicating excellent generalization capability.\n",
        "* It showed perfect precision of 100% in training and 90.5% in testing, suggesting its ability to accurately classify positive instances.\n",
        "* The model achieved perfect recall of 100% in training and 84.7% in testing, indicating it effectively identifies true positive cases.\n",
        "* With a perfect F1 score of 100% in training and 87.5% in testing, it successfully balances precision and recall.\n",
        "* The model's excellent discriminative power is evident with a perfect ROC AUC of 100% in training and 87.9% in testing."
      ],
      "metadata": {
        "id": "jtFSckxEajXZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model 4: KNN (K-Nearest Neighbours)"
      ],
      "metadata": {
        "id": "BdvjL9S8Pc3h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a KNeighborsClassifier object\n",
        "knn = KNeighborsClassifier()\n",
        "\n",
        "# Define the grid of hyperparameters to search\n",
        "param_grid = {\n",
        "    'n_neighbors': [3, 5, 7],  # Number of neighbors to use in the classification\n",
        "    'weights': ['uniform', 'distance'],  # Weight function used in prediction\n",
        "    'p': [1, 2]  # Power parameter for the Minkowski metric (1 for Manhattan distance, 2 for Euclidean distance)\n",
        "}\n",
        "\n",
        "# Create a GridSearchCV object\n",
        "knn_cv = GridSearchCV(knn, param_grid=param_grid, scoring='roc_auc', cv=5)\n",
        "\n",
        "# Fit the GridSearchCV object to the training dataset\n",
        "knn_cv.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "PUSQdWjJPcL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "KNN_test_pred = knn_cv.predict(X_test)\n",
        "KNN_train_pred = knn_cv.predict(X_train)"
      ],
      "metadata": {
        "id": "C3cstI_nQFVY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_model(y_train,KNN_train_pred,y_test,KNN_test_pred,'KNN')"
      ],
      "metadata": {
        "id": "EpukwcjdQW1u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_confusion_matrix(y_test,KNN_test_pred,[True,False])"
      ],
      "metadata": {
        "id": "RdvBGnEeQktf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* The K-Nearest Neighbors (KNN) model achieved perfect training accuracy of 100% and a high testing accuracy of 87.5%, indicating good generalization performance.\n",
        "* It showed perfect precision of 100% in training and 85.2% in testing, suggesting its ability to accurately classify positive instances.\n",
        "* The model achieved perfect recall of 100% in training and 90.6% in testing, indicating its effectiveness in identifying true positive cases.\n",
        "* With a perfect F1 score of 100% in training and 87.8% in testing, it successfully balances precision and recall.\n",
        "* The model's ROC AUC score was excellent, being 100% in training and 87.5% in testing, indicating strong discriminative power."
      ],
      "metadata": {
        "id": "22CI67x5avxc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model 5: **SVM (support vector machine)**"
      ],
      "metadata": {
        "id": "0YOjBBinQ6ln"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Support Vector Machine object\n",
        "svm_model = SVC(kernel='rbf', C=1.0, gamma='scale', probability=True)\n",
        "\n",
        "# Train the SVM model on the training data\n",
        "svm_model.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "NrG7uQzzRCNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SVM_test_pred = svm_model.predict(X_test)\n",
        "SVM_train_pred = svm_model.predict(X_train)"
      ],
      "metadata": {
        "id": "Y8yK9f6BRFhD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_model(y_train,SVM_train_pred,y_test,SVM_test_pred,'SVM')"
      ],
      "metadata": {
        "id": "DxYVQLyCRRQ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_confusion_matrix(y_test,SVM_test_pred,[True,False])"
      ],
      "metadata": {
        "id": "pOCgX2_IRfAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* The Support Vector Machine (SVM) model demonstrated an accuracy of 85.7% in training and 82.9% in testing, indicating decent generalization performance.\n",
        "* SVM exhibited good precision of 91.7% in training and 88.4% in testing, suggesting its ability to correctly classify positive instances.\n",
        "* The model's recall was 78.4% in training and 75.7% in testing, indicating its capability to identify true positive cases.\n",
        "* SVM achieved a balanced F1 score of 84.5% in training and 81.5% in testing, effectively combining precision and recall metrics.\n",
        "* The ROC AUC score was 85.7% in training and 82.9% in testing, indicating satisfactory discriminative power."
      ],
      "metadata": {
        "id": "It08KBbFa7Pz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model 6: **XG Boost**"
      ],
      "metadata": {
        "id": "iauAp6qJR6wP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an XGBoostClassifier object\n",
        "xgb_model = xgb.XGBClassifier()\n",
        "\n",
        "# Define the grid of hyperparameters to search\n",
        "param_grid = param_grid = {\n",
        "    'n_estimators': [100, 200],  # Reduce to two values\n",
        "    'learning_rate': [0.1],      # Keep only one value\n",
        "    'max_depth': [3, 4],         # Reduce to two values\n",
        "    'subsample': [0.8, 0.9],     # Reduce to two values\n",
        "    'min_child_weight': [1, 2]   # Reduce to two values\n",
        "}\n",
        "\n",
        "# Create a GridSearchCV object for XGBoost\n",
        "xgb_cv = GridSearchCV(xgb_model, param_grid=param_grid, scoring='roc_auc', cv=5)\n",
        "\n",
        "# Fit the GridSearchCV object to the training dataset\n",
        "xgb_cv.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "D-pLVrywSCgh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xgb_test_pred = xgb_cv.predict(X_test)\n",
        "xgb_train_pred = xgb_cv.predict(X_train)"
      ],
      "metadata": {
        "id": "EgtmaEFgTPc9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_model(y_train,xgb_train_pred,y_test,xgb_test_pred,'XG boost')"
      ],
      "metadata": {
        "id": "4-09gsTPTZJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_confusion_matrix(y_test,xgb_test_pred,[True,False])"
      ],
      "metadata": {
        "id": "-I5N0qjuTkzs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The XGBoost model exhibited exceptional performance with a training accuracy of 94.5% and a testing accuracy of 88.0%, showcasing its ability to generalize well.\n",
        "* XGBoost demonstrated excellent precision of 98.5% in training and 91.4% in testing, indicating its proficiency in correctly classifying positive instances.\n",
        "* The model's recall was 90.5% in training and 83.9% in testing, suggesting its capability to identify true positive cases.\n",
        "* XGBoost achieved a balanced F1 score of 94.3% in training and 87.5% in testing, effectively combining precision and recall metrics.\n",
        "* The ROC AUC score was 94.5% in training and 88.0% in testing, indicating its strong discriminative power."
      ],
      "metadata": {
        "id": "UDhQC1fHbGd1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Evaluation Results**"
      ],
      "metadata": {
        "id": "EVMA0j7X40dZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation_results"
      ],
      "metadata": {
        "id": "QGIKB063ZQjJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the evaluation results, we can observe that all models perform reasonably well, achieving high accuracy scores on both the training and test datasets. The models have also demonstrated good precision, recall, and F1 scores, indicating a balanced performance between positive and negative classes. However, certain differences stand out among the models.\n",
        "\n",
        "* Logistic Regression: This model exhibits stable performance, with accuracy, precision, recall, and F1 scores consistently high across both training and test datasets. It seems to generalize well and avoid overfitting.\n",
        "\n",
        "* Decision Tree: Although achieving perfect accuracy on the training dataset, the model shows some signs of overfitting as it performs slightly worse on the test dataset. Nevertheless, it still provides good predictive power.\n",
        "\n",
        "* Random Forest: Similar to the Decision Tree, the Random Forest model displays overfitting tendencies. However, it delivers a robust performance on both datasets with high accuracy and precision.\n",
        "\n",
        "* K-Nearest Neighbors (KNN): The KNN model shows signs of overfitting due to its perfect accuracy on the training dataset. Yet, it performs quite well on the test dataset, making it a promising choice.\n",
        "\n",
        "* Support Vector Machine (SVM): The SVM model demonstrates stable performance and generalization, with accuracy, precision, recall, and F1 scores at consistent levels between training and test datasets.\n",
        "\n",
        "* XGBoost: The XGBoost model showcases excellent performance, achieving high accuracy and precision. It demonstrates strong generalization capabilities with minimal overfitting.\n",
        "\n",
        "Considering the overall performance, XGBoost emerges as the best-performing model due to its high accuracy, precision, recall, F1 score, and ROC AUC on both training and test datasets. While other models also exhibit strong performances, XGBoost stands out as the most robust and reliable choice for this classification task."
      ],
      "metadata": {
        "id": "z8yRYQOoDeLC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Dataset Exploration: The dataset provided valuable information on various demographic, behavioral, and medical history factors related to heart health, enabling us to gain insights into CHD risk factors.\n",
        "\n",
        "2. Data Preprocessing: We successfully handled missing values, outliers, and encoded categorical variables to prepare the data for model training.\n",
        "\n",
        "3. Model Selection: Several machine learning models, including Logistic Regression, Decision Tree, Random Forest, KNN, SVM, and XGBoost, were evaluated to predict CHD risk.\n",
        "\n",
        "4. Model Evaluation: The XGBoost model demonstrated superior performance with high accuracy, precision, recall, F1 score, and ROC AUC on both training and test datasets.\n",
        "\n",
        "5. Interpretability: SHAP analysis provided interpretable insights into feature importance, enhancing our understanding of the impact of different factors on CHD risk.\n",
        "\n",
        "6. Practical Implications: The developed XGBoost model can aid medical practitioners in identifying patients at higher risk of CHD and tailoring preventive interventions.\n",
        "\n",
        "7. Public Health Significance: This project contributes to advancing knowledge on cardiovascular diseases and supports public health strategies to combat CHD.\n",
        "\n",
        "8. Future Directions: Further research could explore additional features and incorporate longitudinal data to enhance predictive capabilities and identify potential new risk factors for CHD."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}